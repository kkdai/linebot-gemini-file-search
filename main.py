from fastapi import Request, FastAPI, HTTPException
import os
import sys
import asyncio
import aiohttp
import aiofiles
import urllib.parse
import subprocess
from pathlib import Path
from typing import Optional

from linebot.models import (
    MessageEvent, TextSendMessage, FileMessage, ImageMessage,
    PostbackEvent, TemplateSendMessage, CarouselTemplate, CarouselColumn,
    PostbackAction, QuickReply, QuickReplyButton, MessageAction,
    FlexSendMessage, BubbleContainer, BoxComponent, TextComponent,
    ButtonComponent, SeparatorComponent, CarouselContainer
)
from linebot.exceptions import InvalidSignatureError
from linebot.aiohttp_async_http_client import AiohttpAsyncHttpClient
from linebot import AsyncLineBotApi, WebhookParser

# Google GenAI imports
from google import genai
from google.genai import types

# Chat Session Manager
from chat_session_manager import ChatSessionManager

# Configuration
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") or ""

# Supported file formats for Google AI File Search API
# Reference: https://ai.google.dev/gemini-api/docs/file-upload
SUPPORTED_FILE_EXTENSIONS = {
    '.pdf', '.txt', '.docx', '.html', '.htm', '.md',
    '.csv', '.xml', '.rtf',
    '.doc',   # .doc will be auto-converted to .docx
    '.ppt',   # .ppt will be auto-converted to .pptx
    '.pptx'   # .pptx is natively supported
}

# File format warnings
UNSUPPORTED_FORMAT_MESSAGE = """
‚ö†Ô∏è Ê™îÊ°àÊ†ºÂºè‰∏çÊîØÊè¥

ÊÇ®‰∏äÂÇ≥ÁöÑÊ™îÊ°àÊ†ºÂºè„Äå{extension}„ÄçÁõÆÂâç‰∏çË¢´ÊîØÊè¥„ÄÇ

‚úÖ ÊîØÊè¥ÁöÑÊ†ºÂºèÔºö
‚Ä¢ PDF (.pdf)
‚Ä¢ Word Êñá‰ª∂ (.doc, .docx - .doc ÊúÉËá™ÂãïËΩâÊèõ)
‚Ä¢ PowerPoint (.ppt, .pptx - .ppt ÊúÉËá™ÂãïËΩâÊèõ)
‚Ä¢ Á¥îÊñáÂ≠ó (.txt)
‚Ä¢ Markdown (.md)
‚Ä¢ HTML (.html, .htm)
‚Ä¢ CSV (.csv)
‚Ä¢ RTF (.rtf)

üí° Âª∫Ë≠∞Ôºö
Ë´ã‰ΩøÁî®ÊîØÊè¥ÁöÑÊ†ºÂºèÈáçÊñ∞‰∏äÂÇ≥ÊÇ®ÁöÑÊ™îÊ°à„ÄÇ
"""

# LINE Bot configuration
channel_secret = os.getenv("ChannelSecret", None)
channel_access_token = os.getenv("ChannelAccessToken", None)

# Validate environment variables
if channel_secret is None:
    print("Specify ChannelSecret as environment variable.")
    sys.exit(1)
if channel_access_token is None:
    print("Specify ChannelAccessToken as environment variable.")
    sys.exit(1)
if not GOOGLE_API_KEY:
    raise ValueError("Please set GOOGLE_API_KEY via env var or code.")

# Initialize GenAI client (Note: File Search API only supports Gemini API, not VertexAI)
client = genai.Client(api_key=GOOGLE_API_KEY)

print("GenAI client initialized successfully.")

# Initialize Chat Session Manager
session_manager = ChatSessionManager(client=client, model_name=MODEL_NAME)
print("Chat Session Manager initialized successfully.")

# Initialize the FastAPI app for LINEBot
app = FastAPI()
client_session = aiohttp.ClientSession()
async_http_client = AiohttpAsyncHttpClient(client_session)
line_bot_api = AsyncLineBotApi(channel_access_token, async_http_client)
parser = WebhookParser(channel_secret)

# Create uploads directory if not exists
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

# Model configuration
MODEL_NAME = "gemini-2.5-flash"

def get_store_name(event) -> str:
    """
    Get the file search store name based on the event source.
    Returns user_id for 1-on-1 chat, group_id for group chat.
    Works with both MessageEvent and PostbackEvent.
    """
    if event.source.type == "user":
        return f"user_{event.source.user_id}"
    elif event.source.type == "group":
        return f"group_{event.source.group_id}"
    elif event.source.type == "room":
        return f"room_{event.source.room_id}"
    else:
        return f"unknown_{event.source.user_id}"


async def show_loading_animation(chat_id: str, loading_seconds: int = 20):
    """
    Show loading animation to improve UX during long operations.
    Uses REST API directly to avoid SDK version issues.

    Args:
        chat_id: User ID or Group ID (reply target)
        loading_seconds: Duration in seconds (5-60, default 20)

    Note: This is a fire-and-forget operation. If it fails, it won't affect the main operation.
    """
    try:
        # Ensure loading_seconds is within valid range (5-60 seconds)
        loading_seconds = max(5, min(60, loading_seconds))

        # Use REST API directly
        import requests
        url = "https://api.line.me/v2/bot/chat/loading/start"
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {channel_access_token}'
        }
        payload = {
            'chatId': chat_id,
            'loadingSeconds': loading_seconds
        }

        # Fire-and-forget: don't wait for response
        response = requests.post(url, headers=headers, json=payload, timeout=5)

        if response.status_code == 200:
            print(f"[INFO] Loading animation started for chat: {chat_id} ({loading_seconds}s)")
        else:
            print(f"[WARNING] Loading animation failed: {response.status_code} - {response.text}")

    except Exception as e:
        print(f"[WARNING] Failed to show loading animation: {e}")
        # Don't fail the main operation if animation fails


def get_reply_target(event: MessageEvent) -> str:
    """
    Get the correct reply target ID based on the message source.
    Returns group_id for group chat, user_id for 1-on-1 chat.
    """
    if event.source.type == "group":
        return event.source.group_id
    elif event.source.type == "room":
        return event.source.room_id
    else:
        return event.source.user_id


def is_bot_mentioned(event: MessageEvent, bot_user_id: str) -> bool:
    """
    Check if the bot is mentioned in a group/room message.
    Returns True for 1-on-1 chat, or if bot is mentioned in group/room.

    Args:
        event: MessageEvent from LINE webhook
        bot_user_id: Bot's user ID (from webhook body's 'destination' field)
    """
    print(f"[DEBUG] is_bot_mentioned called, bot_user_id: {bot_user_id}")
    print(f"[DEBUG] event.source.type: {event.source.type}")

    # In 1-on-1 chat, always respond
    if event.source.type == "user":
        print(f"[DEBUG] 1-on-1 chat detected, returning True")
        return True

    # In group/room, check if bot is mentioned
    print(f"[DEBUG] Group/room chat detected")
    print(f"[DEBUG] hasattr(event.message, 'mention'): {hasattr(event.message, 'mention')}")

    if hasattr(event.message, 'mention'):
        print(f"[DEBUG] event.message.mention: {event.message.mention}")
        if event.message.mention:
            mentionees = event.message.mention.mentionees
            print(f"[DEBUG] Number of mentionees: {len(mentionees) if mentionees else 0}")

            if mentionees:
                for i, mentionee in enumerate(mentionees):
                    print(f"[DEBUG] Mentionee {i}:")
                    print(f"[DEBUG]   type: {type(mentionee)}")
                    print(f"[DEBUG]   hasattr user_id: {hasattr(mentionee, 'user_id')}")
                    if hasattr(mentionee, 'user_id'):
                        print(f"[DEBUG]   user_id value: {mentionee.user_id}")
                        print(f"[DEBUG]   Comparing with bot_user_id: {bot_user_id}")

                    # Check if this mention is for the bot by comparing user_id
                    # LINE SDK's Mentionee doesn't have isSelf attribute, so we compare user_id directly
                    if hasattr(mentionee, 'user_id') and mentionee.user_id == bot_user_id:
                        print(f"[DEBUG] Bot mentioned! (user_id matches)")
                        return True

    print(f"[DEBUG] Bot not mentioned, returning False")
    return False


async def download_line_content(message_id: str, file_name: str) -> Optional[Path]:
    """
    Download file content from LINE and save to local uploads directory.
    Returns the local file path if successful, None otherwise.
    """
    try:
        # Get message content from LINE
        message_content = await line_bot_api.get_message_content(message_id)

        # Extract file extension from original file name
        _, ext = os.path.splitext(file_name)
        # Use safe file name (ASCII only) to avoid encoding issues
        safe_file_name = f"{message_id}{ext}"
        file_path = UPLOAD_DIR / safe_file_name

        async with aiofiles.open(file_path, 'wb') as f:
            async for chunk in message_content.iter_content():
                await f.write(chunk)

        print(f"Downloaded file: {file_path} (original: {file_name})")
        return file_path
    except Exception as e:
        print(f"Error downloading file: {e}")
        return None


def is_supported_file_format(file_name: str) -> tuple[bool, str]:
    """
    Check if the file format is supported by Google AI File Search API.
    Returns (is_supported, file_extension).
    """
    _, ext = os.path.splitext(file_name.lower())
    return (ext in SUPPORTED_FILE_EXTENSIONS, ext)


def convert_doc_to_docx(input_path: Path) -> tuple[bool, Path | None, str]:
    """
    Convert .doc file to .docx using LibreOffice.

    Args:
        input_path: Path to the .doc file

    Returns:
        (success, converted_path, message):
            - success: True if conversion succeeded
            - converted_path: Path to the converted .docx file (or None if failed)
            - message: Status or error message
    """
    try:
        # Check if LibreOffice is installed
        libreoffice_commands = ['soffice', 'libreoffice']
        libreoffice_path = None

        for cmd in libreoffice_commands:
            result = subprocess.run(['which', cmd], capture_output=True, text=True)
            if result.returncode == 0:
                libreoffice_path = cmd
                break

        if not libreoffice_path:
            return False, None, "LibreOffice Êú™ÂÆâË£ù"

        # Prepare output directory and expected output file path
        output_dir = input_path.parent
        base_name = input_path.stem  # filename without extension
        expected_output = output_dir / f"{base_name}.docx"

        # Remove existing output file if it exists
        if expected_output.exists():
            expected_output.unlink()

        # Run LibreOffice conversion
        print(f"[INFO] Converting {input_path.name} to .docx using LibreOffice...")
        result = subprocess.run([
            libreoffice_path,
            '--headless',
            '--convert-to', 'docx',
            '--outdir', str(output_dir),
            str(input_path)
        ], capture_output=True, text=True, timeout=60)

        # Check if conversion succeeded
        if result.returncode == 0 and expected_output.exists():
            print(f"[SUCCESS] Converted to: {expected_output.name}")
            return True, expected_output, "ËΩâÊèõÊàêÂäü"
        else:
            error_msg = result.stderr or result.stdout or "Êú™Áü•ÈåØË™§"
            print(f"[ERROR] Conversion failed: {error_msg}")
            return False, None, f"ËΩâÊèõÂ§±ÊïóÔºö{error_msg}"

    except subprocess.TimeoutExpired:
        return False, None, "ËΩâÊèõË∂ÖÊôÇÔºàÊ™îÊ°àÂèØËÉΩÂ§™Â§ßÔºâ"
    except Exception as e:
        print(f"[ERROR] Exception during conversion: {e}")
        return False, None, f"ËΩâÊèõÈåØË™§Ôºö{str(e)}"


def convert_ppt_to_pptx(input_path: Path) -> tuple[bool, Path | None, str]:
    """
    Convert .ppt file to .pptx using LibreOffice.

    Args:
        input_path: Path to the .ppt file

    Returns:
        (success, converted_path, message):
            - success: True if conversion succeeded
            - converted_path: Path to the converted .pptx file (or None if failed)
            - message: Status or error message
    """
    try:
        # Check if LibreOffice is installed
        libreoffice_commands = ['soffice', 'libreoffice']
        libreoffice_path = None

        for cmd in libreoffice_commands:
            result = subprocess.run(['which', cmd], capture_output=True, text=True)
            if result.returncode == 0:
                libreoffice_path = cmd
                break

        if not libreoffice_path:
            return False, None, "LibreOffice Êú™ÂÆâË£ù"

        # Prepare output directory and expected output file path
        output_dir = input_path.parent
        base_name = input_path.stem  # filename without extension
        expected_output = output_dir / f"{base_name}.pptx"

        # Remove existing output file if it exists
        if expected_output.exists():
            expected_output.unlink()

        # Run LibreOffice conversion
        print(f"[INFO] Converting {input_path.name} to .pptx using LibreOffice...")
        result = subprocess.run([
            libreoffice_path,
            '--headless',
            '--convert-to', 'pptx',
            '--outdir', str(output_dir),
            str(input_path)
        ], capture_output=True, text=True, timeout=120)  # PPT files may be larger, give 120 seconds

        # Check if conversion succeeded
        if result.returncode == 0 and expected_output.exists():
            print(f"[SUCCESS] Converted to: {expected_output.name}")
            return True, expected_output, "ËΩâÊèõÊàêÂäü"
        else:
            error_msg = result.stderr or result.stdout or "Êú™Áü•ÈåØË™§"
            print(f"[ERROR] Conversion failed: {error_msg}")
            return False, None, f"ËΩâÊèõÂ§±ÊïóÔºö{error_msg}"

    except subprocess.TimeoutExpired:
        return False, None, "ËΩâÊèõË∂ÖÊôÇÔºàÊ™îÊ°àÂèØËÉΩÂ§™Â§ßÊàñÂÖßÂÆπË§áÈõúÔºâ"
    except Exception as e:
        print(f"[ERROR] Exception during conversion: {e}")
        return False, None, f"ËΩâÊèõÈåØË™§Ôºö{str(e)}"


async def ensure_file_search_store_exists(store_name: str) -> tuple[bool, str]:
    """
    Ensure file search store exists, create if not.
    Returns (success, actual_store_name).
    Note: store_name is used as display_name, but actual name is auto-generated by API.
    """
    try:
        # List all stores and check if one with our display_name exists
        stores = client.file_search_stores.list()
        for store in stores:
            if hasattr(store, 'display_name') and store.display_name == store_name:
                print(f"File search store '{store_name}' already exists: {store.name}")
                return True, store.name

        # Store doesn't exist, create it
        print(f"Creating file search store with display_name '{store_name}'...")
        store = client.file_search_stores.create(
            config={'display_name': store_name}
        )
        print(f"File search store created: {store.name} (display_name: {store_name})")
        return True, store.name

    except Exception as e:
        print(f"Error ensuring file search store exists: {e}")
        return False, ""


# Cache to store display_name -> actual_name mapping
store_name_cache = {}

# Cache to store citations/grounding metadata for each user/group
# Key: store_name, Value: list of grounding chunks
citations_cache = {}


async def list_documents_in_store(store_name: str) -> list:
    """
    List all documents in a file search store.
    Returns list of document info dicts.
    """
    try:
        print(f"[DEBUG] list_documents_in_store called with store_name: {store_name}")

        # Get actual store name
        actual_store_name = None
        if store_name in store_name_cache:
            actual_store_name = store_name_cache[store_name]
            print(f"[DEBUG] Found in cache: {actual_store_name}")
        else:
            # Find store by display_name
            print(f"[DEBUG] Not in cache, searching for store with display_name: {store_name}")
            stores = client.file_search_stores.list()
            print(f"[DEBUG] Total stores found: {len(list(stores))}")

            stores = client.file_search_stores.list()  # Re-list because iterator consumed
            for store in stores:
                store_display_name = getattr(store, 'display_name', None)
                print(f"[DEBUG] Checking store: {store.name}, display_name: {store_display_name}")
                if hasattr(store, 'display_name') and store.display_name == store_name:
                    actual_store_name = store.name
                    store_name_cache[store_name] = actual_store_name
                    print(f"[DEBUG] Found matching store: {actual_store_name}")
                    break

        if not actual_store_name:
            print(f"[DEBUG] Store '{store_name}' not found - returning empty list")
            return []

        print(f"[DEBUG] Using store: {actual_store_name}")

        documents = []

        # Use REST API to list documents (more stable than SDK)
        print(f"[DEBUG] Using REST API to list documents")
        import requests
        url = f"https://generativelanguage.googleapis.com/v1beta/{actual_store_name}/documents"
        headers = {'Content-Type': 'application/json'}
        params = {'key': GOOGLE_API_KEY}

        print(f"[DEBUG] REST API URL: {url}")
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()

        print(f"[DEBUG] REST API returned {len(data.get('documents', []))} documents")

        for doc in data.get('documents', []):
            documents.append({
                'name': doc.get('name', 'N/A'),
                'display_name': doc.get('displayName', 'Unknown'),
                'create_time': doc.get('createTime', ''),
                'update_time': doc.get('updateTime', '')
            })
            print(f"[DEBUG] File found in store '{store_name}': {doc.get('displayName', 'Unknown')}")

        print(f"[DEBUG] Returning {len(documents)} documents")
        return documents

    except Exception as e:
        print(f"[ERROR] Error listing documents in store: {e}")
        import traceback
        traceback.print_exc()
        return []


async def delete_document(document_name: str) -> bool:
    """
    Delete a document from file search store.
    Returns True if successful, False otherwise.
    Note: force=True is required to permanently delete documents from File Search Store.
    """
    try:
        # Try to use SDK method first with force=True
        try:
            if hasattr(client.file_search_stores, 'documents'):
                # Force delete is required for File Search Store documents
                client.file_search_stores.documents.delete(
                    name=document_name,
                    config={'force': True}
                )
                print(f"Document deleted successfully with force=True: {document_name}")
                return True
        except Exception as sdk_error:
            print(f"SDK delete failed, trying REST API: {sdk_error}")

        # Fallback to REST API with force parameter
        import requests
        url = f"https://generativelanguage.googleapis.com/v1beta/{document_name}"
        headers = {'Content-Type': 'application/json'}
        params = {
            'key': GOOGLE_API_KEY,
            'force': 'true'  # Required for File Search Store documents
        }

        response = requests.delete(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()

        print(f"Document deleted successfully via REST API with force=true: {document_name}")
        return True

    except Exception as e:
        print(f"Error deleting document: {e}")
        return False


async def upload_to_file_search_store(file_path: Path, store_name: str, display_name: Optional[str] = None) -> bool:
    """
    Upload a file to Gemini file search store.
    Returns True if successful, False otherwise.
    """
    try:
        # Check cache first
        if store_name in store_name_cache:
            actual_store_name = store_name_cache[store_name]
            print(f"Using cached store name: {actual_store_name}")
        else:
            # Ensure the store exists before uploading
            success, actual_store_name = await ensure_file_search_store_exists(store_name)
            if not success:
                print(f"Failed to ensure store '{store_name}' exists")
                return False
            # Cache the mapping
            store_name_cache[store_name] = actual_store_name

        # Upload to file search store
        # actual_store_name is the API-generated name (e.g., fileSearchStores/xxx)
        # display_name is the custom display name for the file (used in citations)
        config_dict = {}
        if display_name:
            config_dict['display_name'] = display_name

        operation = client.file_search_stores.upload_to_file_search_store(
            file_search_store_name=actual_store_name,
            file=str(file_path),
            config=config_dict if config_dict else None
        )

        # Wait for operation to complete (with timeout)
        max_wait = 60  # seconds
        elapsed = 0
        while not operation.done and elapsed < max_wait:
            await asyncio.sleep(2)
            operation = client.operations.get(operation)
            elapsed += 2

        if operation.done:
            print(f"File uploaded to store '{store_name}': {operation}")
            return True
        else:
            print(f"Upload operation timeout for store '{store_name}'")
            return False

    except Exception as e:
        error_msg = str(e)
        print(f"Error uploading to file search store: {error_msg}")

        # Check if it's a file format related error
        if '500' in error_msg or 'INTERNAL' in error_msg:
            print(f"[WARNING] Possible unsupported file format or corrupted file: {file_path}")
            print(f"[INFO] File extension: {file_path.suffix}")

        return False


async def query_file_search(query: str, store_name: str) -> tuple[str, list]:
    """
    Query the file search store using generate_content.
    Returns (AI response text, list of citations).

    Note: This is the legacy stateless query method.
    For conversation memory, use query_file_search_with_session() instead.
    """
    try:
        # Get actual store name from cache or by searching
        actual_store_name = None

        if store_name in store_name_cache:
            actual_store_name = store_name_cache[store_name]
            print(f"Using cached store name for query: {actual_store_name}")
        else:
            # Try to find the store by display_name
            try:
                stores = client.file_search_stores.list()
                for store in stores:
                    if hasattr(store, 'display_name') and store.display_name == store_name:
                        actual_store_name = store.name
                        store_name_cache[store_name] = actual_store_name
                        print(f"Found store for query: {actual_store_name}")
                        break
            except Exception as list_error:
                print(f"Error listing stores: {list_error}")

        if not actual_store_name:
            # Store doesn't exist - guide user to upload files
            print(f"File search store '{store_name}' not found")
            return ("üìÅ ÊÇ®ÈÇÑÊ≤íÊúâ‰∏äÂÇ≥‰ªª‰ΩïÊ™îÊ°à„ÄÇ\n\nË´ãÂÖàÂÇ≥ÈÄÅÊñá‰ª∂Ê™îÊ°àÔºàPDF„ÄÅDOCX„ÄÅTXT Á≠âÔºâÁµ¶ÊàëÔºå‰∏äÂÇ≥ÂÆåÊàêÂæåÂ∞±ÂèØ‰ª•ÈñãÂßãÊèêÂïè‰∫ÜÔºÅ\n\nüí° ÊèêÁ§∫ÔºöÂ¶ÇÊûúÊÇ®ÊÉ≥ÂàÜÊûêÂúñÁâáÔºåË´ãÁõ¥Êé•ÂÇ≥ÈÄÅÂúñÁâáÁµ¶ÊàëÔºåÊàëÊúÉÁ´ãÂç≥ÁÇ∫ÊÇ®ÂàÜÊûê„ÄÇ", [])

        # Create FileSearch tool with actual store name
        tool = types.Tool(
            file_search=types.FileSearch(
                file_search_store_names=[actual_store_name]
            )
        )

        # Generate content with file search
        response = client.models.generate_content(
            model=MODEL_NAME,
            contents=query,
            config=types.GenerateContentConfig(
                tools=[tool],
                temperature=0.7,
            )
        )

        # Extract grounding metadata (citations)
        citations = []
        try:
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                    grounding_chunks = candidate.grounding_metadata.grounding_chunks
                    for chunk in grounding_chunks:
                        if hasattr(chunk, 'web') and chunk.web:
                            # Web source
                            citations.append({
                                'type': 'web',
                                'title': getattr(chunk.web, 'title', 'Unknown'),
                                'uri': getattr(chunk.web, 'uri', ''),
                            })
                        elif hasattr(chunk, 'retrieved_context') and chunk.retrieved_context:
                            # File search source
                            citations.append({
                                'type': 'file',
                                'title': getattr(chunk.retrieved_context, 'title', 'Unknown'),
                                'text': getattr(chunk.retrieved_context, 'text', '')[:500],  # Limit to 500 chars
                            })
            print(f"Found {len(citations)} citations")
        except Exception as citation_error:
            print(f"Error extracting citations: {citation_error}")

        # Extract text from response
        if response.text:
            return (response.text, citations)
        else:
            return ("Êä±Ê≠âÔºåÊàëÁÑ°Ê≥ïÂæûÊñá‰ª∂‰∏≠ÊâæÂà∞Áõ∏ÈóúË≥áË®ä„ÄÇ", [])

    except Exception as e:
        print(f"Error querying file search: {e}")
        # Check if error is related to missing store
        if "not found" in str(e).lower() or "does not exist" in str(e).lower():
            return ("üìÅ ÊÇ®ÈÇÑÊ≤íÊúâ‰∏äÂÇ≥‰ªª‰ΩïÊ™îÊ°à„ÄÇ\n\nË´ãÂÖàÂÇ≥ÈÄÅÊñá‰ª∂Ê™îÊ°àÔºàPDF„ÄÅDOCX„ÄÅTXT Á≠âÔºâÁµ¶ÊàëÔºå‰∏äÂÇ≥ÂÆåÊàêÂæåÂ∞±ÂèØ‰ª•ÈñãÂßãÊèêÂïè‰∫ÜÔºÅ\n\nüí° ÊèêÁ§∫ÔºöÂ¶ÇÊûúÊÇ®ÊÉ≥ÂàÜÊûêÂúñÁâáÔºåË´ãÁõ¥Êé•ÂÇ≥ÈÄÅÂúñÁâáÁµ¶ÊàëÔºåÊàëÊúÉÁ´ãÂç≥ÁÇ∫ÊÇ®ÂàÜÊûê„ÄÇ", [])
        return (f"Êü•Ë©¢ÊôÇÁôºÁîüÈåØË™§Ôºö{str(e)}", [])


async def query_file_search_with_session(query: str, user_id: str, store_name: str) -> tuple[str, list]:
    """
    Query using ADK Chat Session with conversation memory.
    Implements Option A: Check if documents exist before enabling File Search.

    Returns (AI response text, list of citations).

    Args:
        query: User's question
        user_id: User ID for session management
        store_name: File search store name (display_name format like "user_xxx")
    """
    try:
        print(f"[INFO] query_file_search_with_session called")
        print(f"[INFO] user_id: {user_id}, store_name: {store_name}")

        # Step 1: Check if user has uploaded any documents
        documents = await list_documents_in_store(store_name)
        print(f"[INFO] Found {len(documents)} documents in store")

        if len(documents) == 0:
            # No documents - prompt user to upload
            print(f"[INFO] No documents found, prompting user to upload")
            return ("üìÅ ÊÇ®ÈÇÑÊ≤íÊúâ‰∏äÂÇ≥‰ªª‰ΩïÊ™îÊ°à„ÄÇ\n\nË´ãÂÖàÂÇ≥ÈÄÅÊñá‰ª∂Ê™îÊ°àÔºàPDF„ÄÅDOCX„ÄÅTXT Á≠âÔºâÁµ¶ÊàëÔºå‰∏äÂÇ≥ÂÆåÊàêÂæåÂ∞±ÂèØ‰ª•ÈñãÂßãÊèêÂïè‰∫ÜÔºÅ\n\nüí° ÊèêÁ§∫ÔºöÂ¶ÇÊûúÊÇ®ÊÉ≥ÂàÜÊûêÂúñÁâáÔºåË´ãÁõ¥Êé•ÂÇ≥ÈÄÅÂúñÁâáÁµ¶ÊàëÔºåÊàëÊúÉÁ´ãÂç≥ÁÇ∫ÊÇ®ÂàÜÊûê„ÄÇ", [])

        # Step 2: Get actual store name (API name, not display name)
        actual_store_name = None
        if store_name in store_name_cache:
            actual_store_name = store_name_cache[store_name]
            print(f"[INFO] Using cached actual store name: {actual_store_name}")
        else:
            # Find store by display_name
            stores = client.file_search_stores.list()
            for store in stores:
                if hasattr(store, 'display_name') and store.display_name == store_name:
                    actual_store_name = store.name
                    store_name_cache[store_name] = actual_store_name
                    print(f"[INFO] Found actual store name: {actual_store_name}")
                    break

        if not actual_store_name:
            print(f"[ERROR] Could not find actual store name for: {store_name}")
            return ("Á≥ªÁµ±ÈåØË™§ÔºöÁÑ°Ê≥ïÊâæÂà∞Êñá‰ª∂Â∫´„ÄÇ", [])

        # Step 3: Get or create chat session with File Search enabled
        print(f"[INFO] Getting or creating session with File Search enabled")
        chat = session_manager.get_or_create_session(
            user_id=user_id,
            store_name=actual_store_name,
            enable_file_search=True
        )

        # Step 4: Send message through chat session
        print(f"[INFO] Sending message to chat session")
        response = chat.send_message(query)

        # Step 5: Extract citations (similar to stateless method)
        citations = []
        try:
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                    grounding_chunks = candidate.grounding_metadata.grounding_chunks
                    for chunk in grounding_chunks:
                        if hasattr(chunk, 'web') and chunk.web:
                            citations.append({
                                'type': 'web',
                                'title': getattr(chunk.web, 'title', 'Unknown'),
                                'uri': getattr(chunk.web, 'uri', ''),
                            })
                        elif hasattr(chunk, 'retrieved_context') and chunk.retrieved_context:
                            citations.append({
                                'type': 'file',
                                'title': getattr(chunk.retrieved_context, 'title', 'Unknown'),
                                'text': getattr(chunk.retrieved_context, 'text', '')[:500],
                            })
            print(f"[INFO] Found {len(citations)} citations")
        except Exception as citation_error:
            print(f"[ERROR] Error extracting citations: {citation_error}")

        # Step 6: Return response
        if response.text:
            print(f"[INFO] Successfully generated response with session")
            return (response.text, citations)
        else:
            return ("Êä±Ê≠âÔºåÊàëÁÑ°Ê≥ïÂæûÊñá‰ª∂‰∏≠ÊâæÂà∞Áõ∏ÈóúË≥áË®ä„ÄÇ", [])

    except Exception as e:
        print(f"[ERROR] Error in query_file_search_with_session: {e}")
        import traceback
        traceback.print_exc()
        return (f"Êü•Ë©¢ÊôÇÁôºÁîüÈåØË™§Ôºö{str(e)}", [])


async def analyze_image_with_gemini(image_path: Path) -> str:
    """
    Analyze image using Gemini's vision capability.
    Returns the analysis result text.
    """
    try:
        # Read image bytes
        with open(image_path, 'rb') as f:
            image_bytes = f.read()

        # Determine MIME type based on file extension
        ext = image_path.suffix.lower()
        mime_type_map = {
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif',
            '.webp': 'image/webp'
        }
        mime_type = mime_type_map.get(ext, 'image/jpeg')

        # Create image part
        image = types.Part.from_bytes(
            data=image_bytes,
            mime_type=mime_type
        )

        # Generate content with image
        response = client.models.generate_content(
            model=MODEL_NAME,
            contents=["Ë´ãË©≥Á¥∞ÊèèËø∞ÈÄôÂºµÂúñÁâáÁöÑÂÖßÂÆπÔºåÂåÖÊã¨‰∏ªË¶ÅÁâ©ÂìÅ„ÄÅÂ†¥ÊôØ„ÄÅÊñáÂ≠óÁ≠âË≥áË®ä„ÄÇ", image],
        )

        if response.text:
            return response.text
        else:
            return "Êä±Ê≠âÔºåÊàëÁÑ°Ê≥ïÂàÜÊûêÈÄôÂºµÂúñÁâá„ÄÇ"

    except Exception as e:
        print(f"Error analyzing image with Gemini: {e}")
        return f"ÂúñÁâáÂàÜÊûêÊôÇÁôºÁîüÈåØË™§Ôºö{str(e)}"


async def handle_image_message(event: MessageEvent, message: ImageMessage):
    """
    Handle image messages - analyze using Gemini vision.
    """
    reply_target = get_reply_target(event)
    file_name = f"image_{message.id}.jpg"

    # Show loading animation (15 seconds for image analysis)
    await show_loading_animation(reply_target, loading_seconds=15)

    # Download image
    reply_msg = TextSendMessage(text="Ê≠£Âú®ÂàÜÊûêÊÇ®ÁöÑÂúñÁâáÔºåË´ãÁ®çÂÄô...")
    await line_bot_api.reply_message(event.reply_token, reply_msg)

    file_path = await download_line_content(message.id, file_name)

    if file_path is None:
        error_msg = TextSendMessage(text="ÂúñÁâá‰∏ãËºâÂ§±ÊïóÔºåË´ãÈáçË©¶„ÄÇ")
        await line_bot_api.push_message(reply_target, error_msg)
        return

    # Analyze image with Gemini
    analysis_result = await analyze_image_with_gemini(file_path)

    # Clean up local file
    try:
        file_path.unlink()
    except Exception as e:
        print(f"Error deleting file: {e}")

    # Send analysis result
    result_msg = TextSendMessage(text=f"üì∏ ÂúñÁâáÂàÜÊûêÁµêÊûúÔºö\n\n{analysis_result}")
    await line_bot_api.push_message(reply_target, result_msg)


async def handle_document_message(event: MessageEvent, message: FileMessage):
    """
    Handle file messages - download and upload to file search store.
    """
    store_name = get_store_name(event)
    reply_target = get_reply_target(event)
    file_name = message.file_name or "unknown_file"

    # Check file format before processing
    is_supported, file_ext = is_supported_file_format(file_name)
    if not is_supported:
        # Send unsupported format message
        error_msg = TextSendMessage(text=UNSUPPORTED_FORMAT_MESSAGE.format(extension=file_ext))
        await line_bot_api.reply_message(event.reply_token, error_msg)
        print(f"[WARNING] Unsupported file format: {file_name} ({file_ext})")
        return

    # Show loading animation based on file type
    # .ppt files need more time (60s), others need 30s
    loading_duration = 60 if file_ext == '.ppt' else 30
    await show_loading_animation(reply_target, loading_seconds=loading_duration)

    # Download file
    reply_msg = TextSendMessage(text="Ê≠£Âú®ËôïÁêÜÊÇ®ÁöÑÊ™îÊ°àÔºåË´ãÁ®çÂÄô...")
    await line_bot_api.reply_message(event.reply_token, reply_msg)

    file_path = await download_line_content(message.id, file_name)

    if file_path is None:
        error_msg = TextSendMessage(text="Ê™îÊ°à‰∏ãËºâÂ§±ÊïóÔºåË´ãÈáçË©¶„ÄÇ")
        await line_bot_api.push_message(reply_target, error_msg)
        return

    # Check if file is .doc and convert to .docx
    converted_file_path = None
    conversion_notice = ""
    if file_ext == '.doc':
        print(f"[INFO] Detected .doc file, attempting conversion: {file_name}")

        # Notify user about conversion
        converting_msg = TextSendMessage(text="üîÑ ÂÅµÊ∏¨Âà∞ .doc Ê†ºÂºèÔºåÊ≠£Âú®Ëá™ÂãïËΩâÊèõÁÇ∫ .docx...")
        await line_bot_api.push_message(reply_target, converting_msg)

        success_convert, converted_path, message_convert = convert_doc_to_docx(file_path)

        if success_convert and converted_path:
            print(f"[SUCCESS] Conversion completed: {converted_path.name}")
            converted_file_path = converted_path
            # Update file_name to use .docx extension
            file_name = file_name.rsplit('.', 1)[0] + '.docx'
            conversion_notice = "\n\nüìù Ë®ªÔºöÊ™îÊ°àÂ∑≤Ëá™ÂãïÂæû .doc ËΩâÊèõÁÇ∫ .docx Ê†ºÂºè"
        else:
            # Conversion failed
            error_msg = TextSendMessage(
                text=f"‚ùå .doc Ê™îÊ°àËΩâÊèõÂ§±Êïó\n\n{message_convert}\n\nÂª∫Ë≠∞ÔºöË´ã‰ΩøÁî® Microsoft Word Â∞áÊ™îÊ°àÂè¶Â≠òÁÇ∫ .docx Ê†ºÂºèÂæåÈáçÊñ∞‰∏äÂÇ≥„ÄÇ"
            )
            await line_bot_api.push_message(reply_target, error_msg)

            # Clean up downloaded file
            try:
                file_path.unlink()
            except Exception as e:
                print(f"Error deleting file: {e}")
            return

    # Check if file is .ppt and convert to .pptx
    elif file_ext == '.ppt':
        print(f"[INFO] Detected .ppt file, attempting conversion: {file_name}")

        # Notify user about conversion
        converting_msg = TextSendMessage(text="üîÑ ÂÅµÊ∏¨Âà∞ .ppt Ê†ºÂºèÔºåÊ≠£Âú®Ëá™ÂãïËΩâÊèõÁÇ∫ .pptx...\n\n‚è≥ PPT Ê™îÊ°àËºÉÂ§ßÔºåËΩâÊèõÂèØËÉΩÈúÄË¶Å 10-30 ÁßíÔºåË´ãÁ®çÂÄô...")
        await line_bot_api.push_message(reply_target, converting_msg)

        success_convert, converted_path, message_convert = convert_ppt_to_pptx(file_path)

        if success_convert and converted_path:
            print(f"[SUCCESS] Conversion completed: {converted_path.name}")
            converted_file_path = converted_path
            # Update file_name to use .pptx extension
            file_name = file_name.rsplit('.', 1)[0] + '.pptx'
            conversion_notice = "\n\nüìä Ë®ªÔºöÊ™îÊ°àÂ∑≤Ëá™ÂãïÂæû .ppt ËΩâÊèõÁÇ∫ .pptx Ê†ºÂºè"
        else:
            # Conversion failed
            error_msg = TextSendMessage(
                text=f"‚ùå .ppt Ê™îÊ°àËΩâÊèõÂ§±Êïó\n\n{message_convert}\n\nÂª∫Ë≠∞ÔºöË´ã‰ΩøÁî® Microsoft PowerPoint Â∞áÊ™îÊ°àÂè¶Â≠òÁÇ∫ .pptx Ê†ºÂºèÂæåÈáçÊñ∞‰∏äÂÇ≥„ÄÇ"
            )
            await line_bot_api.push_message(reply_target, error_msg)

            # Clean up downloaded file
            try:
                file_path.unlink()
            except Exception as e:
                print(f"Error deleting file: {e}")
            return

    # Use converted file if available, otherwise use original
    upload_file_path = converted_file_path if converted_file_path else file_path

    # Upload to file search store
    success = await upload_to_file_search_store(upload_file_path, store_name, file_name)

    # Clean up local files
    try:
        file_path.unlink()
        if converted_file_path and converted_file_path != file_path:
            converted_file_path.unlink()
    except Exception as e:
        print(f"Error deleting file: {e}")

    if success:
        # Create Quick Reply buttons for common actions with specific file name
        # Using Postback instead of MessageAction for better Group chat support
        quick_reply = QuickReply(items=[
            QuickReplyButton(action=PostbackAction(
                label="üìù ÁîüÊàêÊ™îÊ°àÊëòË¶Å",
                data=f"action=query&prompt={urllib.parse.quote(f'Ë´ãÂπ´ÊàëÁîüÊàê„Äå{file_name}„ÄçÈÄôÂÄãÊ™îÊ°àÁöÑÊëòË¶Å')}"
            )),
            QuickReplyButton(action=PostbackAction(
                label="üìå ÈáçÈªûÊï¥ÁêÜ",
                data=f"action=query&prompt={urllib.parse.quote(f'Ë´ãÂπ´ÊàëÊï¥ÁêÜ„Äå{file_name}„ÄçÁöÑÈáçÈªû')}"
            )),
            QuickReplyButton(action=PostbackAction(
                label="üìã ÂàóÂá∫Ê™îÊ°à",
                data="action=list_files"
            )),
        ])

        success_msg = TextSendMessage(
            text=f"‚úÖ Ê™îÊ°àÂ∑≤ÊàêÂäü‰∏äÂÇ≥ÔºÅ\nÊ™îÊ°àÂêçÁ®±Ôºö{file_name}{conversion_notice}\n\nÁèæÂú®ÊÇ®ÂèØ‰ª•Ë©¢ÂïèÊàëÈóúÊñºÈÄôÂÄãÊ™îÊ°àÁöÑ‰ªª‰ΩïÂïèÈ°å„ÄÇ",
            quick_reply=quick_reply
        )
        await line_bot_api.push_message(reply_target, success_msg)
    else:
        # Provide more helpful error message
        error_text = f"""‚ùå Ê™îÊ°à‰∏äÂÇ≥Â§±Êïó

Ê™îÊ°àÂêçÁ®±Ôºö{file_name}

ÂèØËÉΩÁöÑÂéüÂõ†Ôºö
1. Ê™îÊ°àÊ†ºÂºèÂèØËÉΩÊúâÂïèÈ°åÊàñÊ™îÊ°àÂ∑≤ÊêçÂ£û
2. Á∂≤Ë∑ØÈÄ£Á∑öÂïèÈ°å
3. Ê™îÊ°àÈÅéÂ§ß

Ë´ãÂòóË©¶Ôºö
‚Ä¢ Á¢∫Ë™çÊ™îÊ°àÂèØ‰ª•Ê≠£Â∏∏ÈñãÂïü
‚Ä¢ Á®çÂæåÈáçË©¶
"""
        error_msg = TextSendMessage(text=error_text)
        await line_bot_api.push_message(reply_target, error_msg)


def is_list_files_intent(text: str) -> bool:
    """
    Check if user wants to list files.
    """
    list_keywords = [
        'ÂàóÂá∫Ê™îÊ°à', 'ÂàóÂá∫Êñá‰ª∂', 'È°ØÁ§∫Ê™îÊ°à', 'È°ØÁ§∫Êñá‰ª∂',
        'Êü•ÁúãÊ™îÊ°à', 'Êü•ÁúãÊñá‰ª∂', 'Ê™îÊ°àÂàóË°®', 'Êñá‰ª∂ÂàóË°®',
        'ÊúâÂì™‰∫õÊ™îÊ°à', 'ÊúâÂì™‰∫õÊñá‰ª∂', 'ÊàëÁöÑÊ™îÊ°à', 'ÊàëÁöÑÊñá‰ª∂',
        'list files', 'show files', 'my files'
    ]
    text_lower = text.lower().strip()
    return any(keyword in text_lower for keyword in list_keywords)


async def send_files_carousel(event, documents: list, page: int = 1, store_name: str = ""):
    """
    Send files as LINE Flex Message Carousel with pagination.
    Works with both MessageEvent and PostbackEvent.

    Args:
        event: MessageEvent or PostbackEvent with reply_token
        documents: List of document dicts with 'name', 'display_name', 'create_time'
        page: Current page number (1-indexed)
        store_name: Store name for pagination postback actions
    """
    if not documents:
        no_files_msg = TextSendMessage(text="üìÅ ÁõÆÂâçÊ≤íÊúâ‰ªª‰ΩïÊñá‰ª∂„ÄÇ\n\nË´ãÂÖà‰∏äÂÇ≥Êñá‰ª∂Ê™îÊ°àÔºåÂ∞±ÂèØ‰ª•Êü•Ë©¢ÂõâÔºÅ")
        await line_bot_api.reply_message(event.reply_token, no_files_msg)
        return

    # ÂàÜÈ†ÅË®≠ÂÆöÔºöÊØèÈ†ÅÊúÄÂ§ö 11 ÂÄãÊ™îÊ°àÔºåÁ¨¨ 12 ÂÄã‰ΩçÁΩÆÁïôÁµ¶ÂàÜÈ†ÅÊéßÂà∂
    page_size = 11
    total_docs = len(documents)
    total_pages = (total_docs + page_size - 1) // page_size  # Âêë‰∏äÂèñÊï¥

    # Ë®àÁÆóÁï∂ÂâçÈ†ÅÁöÑÊ™îÊ°àÁØÑÂúç
    start_idx = (page - 1) * page_size
    end_idx = min(start_idx + page_size, total_docs)
    current_page_docs = documents[start_idx:end_idx]

    print(f"[DEBUG] Pagination: page={page}, total_docs={total_docs}, total_pages={total_pages}")
    print(f"[DEBUG] Showing documents {start_idx+1} to {end_idx}")

    bubbles = []
    for doc in current_page_docs:
        # ÊèêÂèñÊ™îÂêçÔºàÂéªÈô§Ë∑ØÂæëÈÉ®ÂàÜÔºâ
        display_name = doc.get('display_name', 'Unknown')
        # Ê†ºÂºèÂåñÊôÇÈñì
        create_time = doc.get('create_time', '')
        if create_time and 'T' in create_time:
            # Á∞°ÂåñÊôÇÈñìÈ°ØÁ§∫ (YYYY-MM-DD HH:MM)
            try:
                from datetime import datetime
                dt = datetime.fromisoformat(create_time.replace('Z', '+00:00'))
                create_time = dt.strftime('%Y-%m-%d %H:%M')
            except:
                create_time = create_time[:16]  # Á∞°ÂñÆÊà™Êñ∑

        # Âª∫Á´ãÊØèÂÄãÊ™îÊ°àÁöÑ Bubble
        bubble = BubbleContainer(
            body=BoxComponent(
                layout='vertical',
                contents=[
                    # Ê™îÊ°àÂúñÁ§∫
                    TextComponent(
                        text='üìÑ',
                        size='xxl',
                        align='center',
                        margin='md'
                    ),
                    # Ê™îÊ°àÂêçÁ®±
                    TextComponent(
                        text=display_name[:40],  # ÈôêÂà∂Èï∑Â∫¶
                        weight='bold',
                        size='lg',
                        align='center',
                        wrap=True,
                        margin='md'
                    ),
                    # ÂàÜÈöîÁ∑ö
                    SeparatorComponent(margin='md'),
                    # ‰∏äÂÇ≥ÊôÇÈñì
                    TextComponent(
                        text=f"‰∏äÂÇ≥ÊôÇÈñì\n{create_time}" if create_time else "Êñá‰ª∂Ê™îÊ°à",
                        size='sm',
                        color='#999999',
                        align='center',
                        wrap=True,
                        margin='md'
                    )
                ],
                padding_all='lg'
            ),
            footer=BoxComponent(
                layout='vertical',
                contents=[
                    # Âà™Èô§ÊåâÈàï
                    ButtonComponent(
                        action=PostbackAction(
                            label='üóëÔ∏è Âà™Èô§Ê™îÊ°à',
                            data=f"action=delete_file&doc_name={doc['name']}"
                        ),
                        style='primary',
                        color='#e74c3c',
                        height='sm'
                    )
                ],
                padding_all='sm'
            )
        )
        bubbles.append(bubble)

    # Âä†ÂÖ•ÂàÜÈ†ÅÊéßÂà∂ bubble (Â¶ÇÊûúÊúâÂ§öÈ†Å)
    if total_pages > 1:
        # Âª∫Á´ãÂàÜÈ†ÅÊåâÈàï
        pagination_buttons = []

        # ‰∏ä‰∏ÄÈ†ÅÊåâÈàï (Â¶ÇÊûú‰∏çÊòØÁ¨¨‰∏ÄÈ†Å)
        if page > 1:
            pagination_buttons.append(
                ButtonComponent(
                    action=PostbackAction(
                        label='‚¨ÖÔ∏è ‰∏ä‰∏ÄÈ†Å',
                        data=f"action=list_files&page={page-1}&store={urllib.parse.quote(store_name)}"
                    ),
                    style='secondary',
                    color='#95a5a6',
                    height='sm'
                )
            )

        # ‰∏ã‰∏ÄÈ†ÅÊåâÈàï (Â¶ÇÊûú‰∏çÊòØÊúÄÂæå‰∏ÄÈ†Å)
        if page < total_pages:
            pagination_buttons.append(
                ButtonComponent(
                    action=PostbackAction(
                        label='‰∏ã‰∏ÄÈ†Å ‚û°Ô∏è',
                        data=f"action=list_files&page={page+1}&store={urllib.parse.quote(store_name)}"
                    ),
                    style='primary',
                    color='#3498db',
                    height='sm'
                )
            )

        # Âª∫Á´ãÂàÜÈ†ÅÊéßÂà∂ bubble
        pagination_bubble = BubbleContainer(
            body=BoxComponent(
                layout='vertical',
                contents=[
                    TextComponent(
                        text='üìÑ',
                        size='xxl',
                        align='center',
                        margin='md'
                    ),
                    TextComponent(
                        text='È†ÅÈù¢Â∞éËà™',
                        weight='bold',
                        size='lg',
                        align='center',
                        margin='md'
                    ),
                    SeparatorComponent(margin='md'),
                    TextComponent(
                        text=f'Á¨¨ {page} / {total_pages} È†Å',
                        size='sm',
                        color='#999999',
                        align='center',
                        margin='md'
                    ),
                    TextComponent(
                        text=f'ÂÖ± {total_docs} ÂÄãÊ™îÊ°à',
                        size='xs',
                        color='#999999',
                        align='center',
                        margin='sm'
                    )
                ],
                padding_all='lg'
            ),
            footer=BoxComponent(
                layout='vertical',
                contents=pagination_buttons,
                spacing='sm',
                padding_all='sm'
            )
        )
        bubbles.append(pagination_bubble)

    # Âª∫Á´ã Carousel Container
    carousel_container = CarouselContainer(contents=bubbles)

    # Âª∫Á´ã Flex Message
    flex_message = FlexSendMessage(
        alt_text=f'üìÅ ÊâæÂà∞ {total_docs} ÂÄãÊñá‰ª∂ (Á¨¨ {page}/{total_pages} È†Å)',
        contents=carousel_container
    )

    await line_bot_api.reply_message(event.reply_token, flex_message)


async def handle_postback(event: PostbackEvent):
    """
    Handle postback events from Quick Reply buttons and other interactions.
    Supports: delete_file, query, list_files, view_citation
    """
    try:
        # Parse postback data
        data = event.postback.data
        params = dict(param.split('=', 1) for param in data.split('&'))

        action = params.get('action')
        print(f"[DEBUG] Postback action: {action}")

        # Get store name for operations
        store_name = get_store_name(event)

        if action == 'delete_file':
            # Handle delete file action
            doc_name = params.get('doc_name')
            if doc_name:
                success = await delete_document(doc_name)

                if success:
                    reply_msg = TextSendMessage(
                        text=f"‚úÖ Ê™îÊ°àÂ∑≤Âà™Èô§ÊàêÂäüÔºÅ\n\nÂ¶ÇÈúÄÊü•ÁúãÂâ©È§òÊ™îÊ°àÔºåË´ãÈªûÊìä‰∏ãÊñπÊåâÈàï„ÄÇ"
                    )
                else:
                    reply_msg = TextSendMessage(text="‚ùå Âà™Èô§Ê™îÊ°àÂ§±ÊïóÔºåË´ãÁ®çÂæåÂÜçË©¶„ÄÇ")

                await line_bot_api.reply_message(event.reply_token, reply_msg)

        elif action == 'query':
            # Handle file query from Quick Reply
            prompt = urllib.parse.unquote(params.get('prompt', ''))
            print(f"[DEBUG] Query prompt: {prompt}")

            if prompt:
                # Query file search
                response_text, citations = await query_file_search(prompt, store_name)

                # Store citations in cache
                if citations:
                    citations_cache[store_name] = citations[:3]
                    print(f"Stored {len(citations_cache[store_name])} citations for {store_name}")

                # Create Quick Reply buttons for citations
                quick_reply = None
                if citations:
                    quick_reply_items = []
                    for i, citation in enumerate(citations[:3], 1):
                        quick_reply_items.append(
                            QuickReplyButton(action=PostbackAction(
                                label=f"üìñ ÂºïÁî®{i}",
                                data=f"action=view_citation&num={i}"
                            ))
                        )
                    quick_reply = QuickReply(items=quick_reply_items)

                # Reply to user
                reply_msg = TextSendMessage(text=response_text, quick_reply=quick_reply)
                await line_bot_api.reply_message(event.reply_token, reply_msg)
            else:
                reply_msg = TextSendMessage(text="Êü•Ë©¢ÂÖßÂÆπ‰∏çËÉΩÁÇ∫Á©∫„ÄÇ")
                await line_bot_api.reply_message(event.reply_token, reply_msg)

        elif action == 'list_files':
            # Handle list files request - show carousel with delete buttons
            # Parse pagination parameters
            page = int(params.get('page', 1))
            store = urllib.parse.unquote(params.get('store', store_name))

            print(f"[DEBUG] Postback list_files action for store: {store}, page: {page}")
            documents = await list_documents_in_store(store)
            print(f"[DEBUG] Postback list_documents_in_store returned {len(documents)} documents")
            await send_files_carousel(event, documents, page=page, store_name=store)

        elif action == 'view_citation':
            # Handle view citation request
            citation_num = int(params.get('num', 0))
            print(f"[DEBUG] View citation {citation_num} for store: {store_name}")

            if store_name in citations_cache and 0 < citation_num <= len(citations_cache[store_name]):
                citation = citations_cache[store_name][citation_num - 1]

                # Format citation text
                if citation['type'] == 'file':
                    citation_text = f"üìñ ÂºïÁî® {citation_num}\n\n"
                    citation_text += f"üìÑ Êñá‰ª∂Ôºö{citation['title']}\n\n"
                    citation_text += f"üìù ÂÖßÂÆπÔºö\n{citation['text']}"
                    if len(citation.get('text', '')) >= 500:
                        citation_text += "\n\n... (ÂÖßÂÆπÈÅéÈï∑ÔºåÂ∑≤Êà™Êñ∑)"
                elif citation['type'] == 'web':
                    citation_text = f"üìñ ÂºïÁî® {citation_num}\n\n"
                    citation_text += f"üåê ‰æÜÊ∫êÔºö{citation['title']}\n"
                    citation_text += f"üîó ÈÄ£ÁµêÔºö{citation['uri']}"
                else:
                    citation_text = "ÁÑ°Ê≥ïÈ°ØÁ§∫Ê≠§ÂºïÁî®„ÄÇ"

                reply_msg = TextSendMessage(text=citation_text)
                await line_bot_api.reply_message(event.reply_token, reply_msg)
            else:
                reply_msg = TextSendMessage(text="Êâæ‰∏çÂà∞Ê≠§ÂºïÁî®ÔºåË´ãÈáçÊñ∞Êü•Ë©¢„ÄÇ")
                await line_bot_api.reply_message(event.reply_token, reply_msg)

        else:
            print(f"Unknown postback action: {action}")
            reply_msg = TextSendMessage(text="Êú™Áü•ÁöÑÊìç‰Ωú„ÄÇ")
            await line_bot_api.reply_message(event.reply_token, reply_msg)

    except Exception as e:
        print(f"Error handling postback: {e}")
        import traceback
        traceback.print_exc()
        error_msg = TextSendMessage(text="ËôïÁêÜÊìç‰ΩúÊôÇÁôºÁîüÈåØË™§„ÄÇ")
        await line_bot_api.reply_message(event.reply_token, error_msg)


async def handle_text_message(event: MessageEvent, message, bot_user_id: str = ''):
    """
    Handle text messages - query the file search store or list files.
    Only responds in groups if bot is mentioned.
    Now uses ADK Chat Session for conversation memory.

    Args:
        event: MessageEvent from LINE webhook
        message: Message object
        bot_user_id: Bot's user ID for mention checking
    """
    # In group/room, only respond if bot is mentioned
    if not is_bot_mentioned(event, bot_user_id):
        print(f"Bot not mentioned in group/room, skipping response")
        return

    store_name = get_store_name(event)
    query = message.text
    user_id = event.source.user_id

    print(f"Received query: {query} for store: {store_name}, user: {user_id}")

    # Check if user wants to clear conversation
    clear_keywords = ['Ê∏ÖÈô§Â∞çË©±', 'Ê∏ÖÈô§ÂØπËØù', 'reset', 'clear', 'ÈáçÁΩÆÂ∞çË©±', 'ÈáçÁΩÆÂØπËØù', 'Ê∏ÖÁ©∫Â∞çË©±', 'Ê∏ÖÁ©∫ÂØπËØù']
    if any(keyword in query.lower() for keyword in clear_keywords):
        print(f"[INFO] Clear session command detected")
        success = session_manager.clear_session(user_id)
        if success:
            reply_msg = TextSendMessage(text="‚úÖ Â∞çË©±Ë®òÊÜ∂Â∑≤Ê∏ÖÈô§„ÄÇ\n\nÊàëÂÄëÂèØ‰ª•ÈáçÊñ∞ÈñãÂßãÂ∞çË©±‰∫ÜÔºÅ")
        else:
            reply_msg = TextSendMessage(text="‚úÖ Â∞çË©±Ë®òÊÜ∂Â∑≤Ê∏ÖÈô§„ÄÇ\n\nÔºàÊÇ®ÁõÆÂâçÊ≤íÊúâÈÄ≤Ë°å‰∏≠ÁöÑÂ∞çË©±Ôºâ")
        await line_bot_api.reply_message(event.reply_token, reply_msg)
        return

    # Note: Citation viewing is now handled by Postback actions
    # The Quick Reply buttons trigger postback events instead of text messages

    # Check if user wants to list files
    if is_list_files_intent(query):
        print(f"[DEBUG] List files intent detected for query: {query}")
        print(f"[DEBUG] Store name: {store_name}")
        # Show files carousel with delete buttons
        documents = await list_documents_in_store(store_name)
        print(f"[DEBUG] list_documents_in_store returned {len(documents)} documents")
        await send_files_carousel(event, documents, page=1, store_name=store_name)
        return

    # Query file search with session (ADK Chat Session with conversation memory)
    print(f"[INFO] Using query_file_search_with_session")
    response_text, citations = await query_file_search_with_session(query, user_id, store_name)

    # Store citations in cache (limit to 3 for Quick Reply)
    if citations:
        citations_cache[store_name] = citations[:3]
        print(f"Stored {len(citations_cache[store_name])} citations for {store_name}")

    # Create Quick Reply buttons for citations
    # Using Postback instead of MessageAction for better Group chat support
    quick_reply = None
    if citations:
        quick_reply_items = []
        for i, citation in enumerate(citations[:3], 1):  # Limit to 3 citations
            quick_reply_items.append(
                QuickReplyButton(action=PostbackAction(
                    label=f"üìñ ÂºïÁî®{i}",
                    data=f"action=view_citation&num={i}"
                ))
            )
        quick_reply = QuickReply(items=quick_reply_items)

    # Reply to user
    reply_msg = TextSendMessage(text=response_text, quick_reply=quick_reply)
    await line_bot_api.reply_message(event.reply_token, reply_msg)


@app.post("/")
async def handle_callback(request: Request):
    signature = request.headers["X-Line-Signature"]

    # Get request body as text
    body = await request.body()
    body = body.decode()

    # Log webhook body for debugging
    print("[DEBUG] ===== Webhook Request =====")
    print(f"[DEBUG] Body: {body}")
    print("[DEBUG] ===========================")

    # Parse body to get bot user ID (destination)
    import json
    try:
        body_json = json.loads(body)
        bot_user_id = body_json.get('destination', '')
        print(f"[DEBUG] Bot User ID (destination): {bot_user_id}")
    except json.JSONDecodeError:
        print("[ERROR] Failed to parse webhook body JSON")
        bot_user_id = ''

    try:
        events = parser.parse(body, signature)
    except InvalidSignatureError:
        raise HTTPException(status_code=400, detail="Invalid signature")

    for event in events:
        print(f"[DEBUG] Event type: {type(event).__name__}")
        print(f"[DEBUG] Event source type: {event.source.type if hasattr(event, 'source') else 'N/A'}")
        # Handle PostbackEvent (e.g., delete file button clicks)
        if isinstance(event, PostbackEvent):
            await handle_postback(event)
        # Handle MessageEvent
        elif isinstance(event, MessageEvent):
            if event.message.type == "text":
                # Process text message (pass bot_user_id for mention checking)
                await handle_text_message(event, event.message, bot_user_id)
            elif event.message.type == "file":
                # Process file message (upload to file search store)
                await handle_document_message(event, event.message)
            elif event.message.type == "image":
                # Process image message (analyze with Gemini vision)
                await handle_image_message(event, event.message)
            else:
                continue
        else:
            continue

    return "OK"


@app.on_event("shutdown")
async def shutdown_event():
    """Clean up resources on shutdown."""
    await client_session.close()
